

## Open source LLMs high in leader boards 


When checking the existing **LLM leaderboards** where we get public rankings, such as the LLM arena ( #source) which is  a crowdsourced battle platform or llm-stat ( #source) which based its leaderboard on standarized benchmarks. 

Among the **open-source** models, **four stand out consistently** for their high performance, accessibility, and reasoning ability:


|                 | Context Window | Size | Reasoning |
| --------------- | -------------- | ---- | --------- |
| DeepSeek R1     | 128k           | 671B |           |
| LLaMA 4         | 1M             | 400B |           |
| Qwen 3          | 128k           | 235B |           |
| Mistral Small 3 | 24B            | 24B  |           |
|                 |                |      |           |



- DeepSeek R1 
- Mistral 
- LLaMA 4
- Qwen 3

In our case, **DeepSeek emerges as the most suitable choice**, as it consistently outperforms the others across multiple evaluation platforms, making it the strongest candidate for our needs in multi-agent collaboration.

